{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `build_w2v_model`() takes several parameters:\n",
    "    - `tokens`: A list of lists of strings, where each inner list represents the tokens in a single document.\n",
    "    - `n_features`: An integer that specifies the dimensionality of the word vectors.\n",
    "    - `seed`: An optional integer that sets the random seed for the Word2Vec model.\n",
    "    - `workers`: An optional integer that specifies the number of worker threads to use when training the model.\n",
    "    - `sg`: An optional integer that specifies the training algorithm: 0 for CBOW, 1 for skip-gram.\n",
    "    - `context_size`: An optional integer that specifies the size of the context window.\n",
    "    - `down_sampling`: An optional float that specifies the threshold for downsampling high-frequency words.\n",
    "    - `min_word_count`: An optional integer that specifies the minimum frequency of a word to be included in the vocabulary.\n",
    "- The function returns a trained Word2Vec model.\n",
    "- The model is trained using the `Word2Vec` class from the `gensim` library.\n",
    "- The `sentences` parameter of the `Word2Vec` class is set to the `tokens` parameter of the `build_w2v_model` function.\n",
    "- The `sg`, `seed`, `workers`, `vector_size`, `min_count`, `window`, and `sample` parameters of the `Word2Vec` class are set to the corresponding parameters of the `build_w2v_model` function.\n",
    "- The trained Word2Vec model can be used to generate word embeddings for words in the vocabulary.\n",
    "\n",
    "- `w2v_embedding`() function takes in two parameters, `tokens`, a list of lists of strings, and `wv`, a word vector object.\n",
    "    - It initializes an empty list called `sentence_vectors`.\n",
    "    - It loops through each list of strings in `tokens`.\n",
    "        - For each string in the list, it retrieves the corresponding word vector from `wv`.\n",
    "        - It adds the word vectors to a list called `word_vectors`.\n",
    "        - It calculates the mean of the `word_vectors` list along the first axis (i.e., the mean of all the word vectors in the list).\n",
    "        - It appends the resulting sentence vector to the `sentence_vectors` list.\n",
    "- Finally, it returns a numpy array containing all the sentence vectors.\n",
    "\n",
    "The purpose of the `w2v_embedding` function is to convert a list of lists of words into a matrix of sentence vectors, where each sentence vector represents the average of the word vectors for the words in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w2v_model(\n",
    "        tokens:list[list[str]],\n",
    "        n_features:int,\n",
    "        seed:int = 1,\n",
    "        workers = 1,\n",
    "        sg:int = 0,\n",
    "        context_size:int = 5,\n",
    "        down_sampling:int = 1e-3,\n",
    "        min_word_count:int = 0) -> w2v:\n",
    "\n",
    "    return w2v.Word2Vec(\n",
    "        sentences=tokens,\n",
    "        sg=sg,\n",
    "        seed=seed,\n",
    "        workers = workers,\n",
    "        vector_size = n_features,\n",
    "        min_count = min_word_count,\n",
    "        window = context_size,\n",
    "        sample = down_sampling\n",
    "    )\n",
    "\n",
    "def w2v_embedding(\n",
    "        tokens: list[list[str]], \n",
    "        wv: w2v.wv) -> np.ndarray:\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    for sentence in tokens:\n",
    "        word_vectors = []\n",
    "        for token in sentence:\n",
    "            word_vectors.append(wv.get_vector(token))\n",
    "        sentence_vectors.append(list(np.mean(word_vectors, axis=0)))\n",
    "\n",
    "    return np.array(sentence_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Cosine Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tfidf_vectorizer()`:\n",
    "    - This function takes in a list of strings (`corpus`) and a maximum number of features to include in the vectorizer (`max_features`).\n",
    "    - It returns a `TfidfVectorizer` object that has been trained on the input corpus.\n",
    "    - The vectorizer converts the corpus of strings into a sparse matrix of TF-IDF features, where each row represents a document and each column represents a unique word in the corpus.\n",
    "    - The TF-IDF values represent how important each word is to each document in the corpus.\n",
    "- `cosine_distance()`:\n",
    "    - This function takes in two vectors (`vector1` and `vector2`) and calculates the cosine similarity between them.\n",
    "    - If the input vectors are sparse matrices, they are first converted to dense arrays.\n",
    "    - The function calculates the dot product of the two vectors after normalizing them to unit vectors using L2 normalization.\n",
    "    - The resulting value represents the cosine of the angle between the two vectors, which is a measure of their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(\n",
    "        corpus: list[str],\n",
    "        max_features=1000) -> TfidfVectorizer:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features)\n",
    "    return vectorizer.fit(corpus)\n",
    "\n",
    "\n",
    "def cosine_distance(\n",
    "        vector1: scipy.sparse.csr.csr_matrix | np.ndarray, \n",
    "        vector2: scipy.sparse.csr.csr_matrix | np.ndarray) -> float:\n",
    "    if isinstance(vector1, scipy.sparse.csr.csr_matrix):\n",
    "        return np.dot(\n",
    "            vector1.T.toarray()[0]/np.linalg.norm(vector1.toarray()),\n",
    "            vector2.T.toarray()[0]/np.linalg.norm(vector2.toarray()))\n",
    "    else:\n",
    "        return np.dot(\n",
    "            vector1/np.linalg.norm(vector1),\n",
    "            vector2/np.linalg.norm(vector2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0\n",
      "rootdir: /Users/eyuelmelese/Desktop/master/NLP/NLP-Quora\n",
      "plugins: anyio-3.6.2\n",
      "collected 4 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "nlp_quora/tests.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.66s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.9 -m pytest tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash Script Docker Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.5s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.7s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.8s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              1.0s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9              1.1s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (8/8) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9              1.1s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/library/python:3.9@sha256:2d8875d28ca023a9056a82  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] RUN git clone https://github.com/sergiohj93/NLP-Quora.gi  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:3085613a840c05d3d7194ac700e687a34d2511951b0f7  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/nlp_quora:latest                        0.0s\n",
      "\u001b[?2004h96f1958b5bc1:/app# 958b5bc1:/app# ^C\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004hroot@96f1958b5bc1:/app# "
     ]
    }
   ],
   "source": [
    "!bash docker-build.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06cdd811",
   "metadata": {},
   "source": [
    "# Word comparing functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96db00d7",
   "metadata": {},
   "source": [
    "As features for the improved model, we considered the comparation of words at two levels: checking if they have the same word in the same position (checking all tokens with the same index), and also checking the key words (what, where, when, who, why...) of a question and looking if they share the first keyword. We implement it and obtain the following features with their respective functions:\n",
    "\n",
    "First, we have the same_words_ordered, which check all tokens on the shorter tokenized question against the tokens of the other comparing the words with the same index. After that, we divide by the length of the longer tokenized question, in order not to ignore the part of the longer question that is not compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a217245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_words_ordered(q1_tokens,q2_tokens):\n",
    "    n = min(len(q1_tokens), len(q2_tokens))\n",
    "    m = max(len(q1_tokens), len(q2_tokens))\n",
    "    same = 0\n",
    "    for i in range(n):\n",
    "        if q1_tokens[i] == q2_tokens[i]:\n",
    "            same += 1\n",
    "\n",
    "    return same / m\n",
    "\n",
    "def test_same_words_ordered():\n",
    "    # Test case 1: Same question\n",
    "    q1_tokens = ['what', 'is', 'the', 'capital', 'of', 'France']\n",
    "    q2_tokens = ['what', 'is', 'the', 'capital', 'of', 'France']\n",
    "    assert same_words_ordered(q1_tokens, q2_tokens) == 1.0\n",
    "\n",
    "    # Test case 2: Different questions with just a f common words\n",
    "    q1_tokens = ['what', 'do', 'you', 'like', 'about', 'France']\n",
    "    q2_tokens = ['who', 'won', 'the', 'World', 'Cup', 'last', 'year']\n",
    "    assert same_words_ordered(q1_tokens, q2_tokens) == 0.0\n",
    "\n",
    "    # Test case 3: Questions with different lengths\n",
    "    q1_tokens = ['what', 'is', 'the', 'capital', 'of', 'France']\n",
    "    q2_tokens = ['what', 'is', 'the', 'capital', 'city', 'of', 'France']\n",
    "    assert same_words_ordered(q1_tokens, q2_tokens) == 4.0 / 7.0\n",
    "\n",
    "        # Test case 4: Different questions with some common words\n",
    "    q1_tokens = ['what', 'is', 'the', 'capital', 'of', 'France']\n",
    "    q2_tokens = ['what', 'is', 'the', 'capital', 'of', 'Spain']\n",
    "    assert same_words_ordered(q1_tokens, q2_tokens) == 5.0 / 6.0\n",
    "\n",
    "\n",
    "test_same_words_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d1acc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 0.5714285714285714]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ordered_words_feature(q1_tokens,q2_tokens):\n",
    "    ow_feature = []\n",
    "    for i in range(len(q1_tokens)):\n",
    "        ow = same_words_ordered(q1_tokens[i],q2_tokens[i])\n",
    "        ow_feature.append(ow)\n",
    "    return ow_feature\n",
    "\n",
    "q1_tokens = [['what', 'is', 'the', 'capital', 'of', 'France'], ['what', 'do', 'you', 'like', 'about', 'France'], ['what', 'is', 'the', 'capital', 'of', 'France']]\n",
    "q2_tokens = [['what', 'is', 'the', 'capital', 'of', 'France'], ['who', 'won', 'the', 'World', 'Cup', 'last', 'year'], ['what', 'is', 'the', 'capital', 'city', 'of', 'France']]\n",
    "\n",
    "generate_ordered_words_feature(q1_tokens, q2_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47d98053",
   "metadata": {},
   "source": [
    "Next, we have the functions to check the keywords. This function is a quick way to check a commonality between some questions: the type of question is asked. We search for the first keyword on each question and then we check if they are the same type of keyword. Some keywords are grouped because they can be interchanged without changing the meaning of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89b15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_key_word(q_tokens):\n",
    "    for token in q_tokens:\n",
    "        if token in ['where', 'when', 'who', 'do', 'should']:\n",
    "            return token\n",
    "        elif token in ['can', 'how', 'could']:\n",
    "            return 'can'\n",
    "        elif token in ['what', 'which']:\n",
    "            return 'what'\n",
    "        # Hard-coded case\n",
    "        elif token in ['why', 'whey']:\n",
    "            return 'why'\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def test_search_key_word():\n",
    "    q_tokens = ['where', 'is', 'the', 'nearest', 'gas', 'station']\n",
    "    assert search_key_word(q_tokens) == 'where'\n",
    "\n",
    "    q_tokens = ['hey', 'should', 'I', 'wear', 'a', 'jacket']\n",
    "    assert search_key_word(q_tokens) == 'should'\n",
    "\n",
    "    q_tokens = ['who', 'is', 'the', 'president', 'of', 'the', 'USA']\n",
    "    assert search_key_word(q_tokens) == 'who'\n",
    "\n",
    "    q_tokens = ['why', 'is', 'the', 'sky', 'blue']\n",
    "    assert search_key_word(q_tokens) == 'why'\n",
    "\n",
    "    q_tokens = ['This', 'is', 'a', 'test', 'question']\n",
    "    assert search_key_word(q_tokens) is None\n",
    "\n",
    "\n",
    "test_search_key_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e772ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_key_word(q1_tokens, q2_tokens):\n",
    "    key_word1 = search_key_word(q1_tokens)\n",
    "    key_word2 = search_key_word(q2_tokens)\n",
    "    if key_word1 and key_word1 == key_word2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def test_compare_key_word():\n",
    "    assert compare_key_word(['what', 'is', 'your', 'name'], ['what', 'is', 'my', 'name']) == 1\n",
    "    assert compare_key_word(['where', 'are', 'you', 'from'], ['what', 'is', 'your', 'name']) == 0\n",
    "    assert compare_key_word(['can', 'you', 'help', 'me'], ['how', 'can', 'I', 'help', 'you']) == 1\n",
    "    assert compare_key_word(['why', 'is', 'the', 'sky', 'blue'], ['what', 'is', 'your', 'name']) == 0\n",
    "    assert compare_key_word(['where', 'can', 'I', 'buy', 'tickets'], ['where', 'should', 'I', 'buy', 'tickets']) == 1\n",
    "    assert compare_key_word(['what', 'is', 'the', 'capital', 'of', 'Spain'], ['which', 'city', 'is', 'the', 'capital', 'of', 'Spain']) == 1\n",
    "    assert compare_key_word(['can', 'I', 'pay', 'with', 'cash'], ['can', 'I', 'pay', 'with', 'credit', 'card']) == 1\n",
    "    assert compare_key_word(['why', 'do', 'birds', 'migrate'], ['how', 'do', 'birds', 'fly']) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f37f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_key_words_feature(q1_tokens, q2_tokens):\n",
    "    kw_feature = []\n",
    "    for i in range(len(q1_tokens)):\n",
    "        kw = compare_key_word(q1_tokens[i],q2_tokens[i])\n",
    "        kw_feature.append(kw)\n",
    "    return kw_feature\n",
    "\n",
    "def test_generate_key_words_feature():\n",
    "    q1_tokens = [['what', 'is', 'your', 'name'], ['where', 'are', 'you', 'from'], ['can', 'you', 'help', 'me'], ['why', 'is', 'the', 'sky', 'blue']]\n",
    "    q2_tokens = [['what', 'is', 'my', 'name'], ['what', 'is', 'my', 'name'], ['how', 'can', 'I', 'help', 'you'], ['what', 'is', 'my', 'name']]\n",
    "    assert generate_key_words_feature(q1_tokens, q2_tokens) == [1, 0, 1, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42d8f855-81cd-43a3-8a4e-20dedeb08b0c",
   "metadata": {},
   "source": [
    "# Discarded functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43f5384c-df11-4fcb-8eaa-1a3c017b054d",
   "metadata": {},
   "source": [
    "We also had another features in mind that we ended up discarding because they were taking a long time to create the features for all the datasets. One example of this was the 'negation' feature, which checks if a question has negation present on it and if two question share this property or not.\n",
    "\n",
    "It took one hour to generate the train features. Therefore, we discarded this feature, and thereby we abandoned other feature ideas that needed the same spacy model, such as grammatical struture, lemmatization and named entities. We also discarded synonyms, because of the same reasons (too much computation time to get features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca29928-aa0d-4f10-9e7f-c52a92e33102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def negation(sent1, sent2, nlp):\n",
    "    doc1 = nlp(sent1)\n",
    "    doc2 = nlp(sent2)\n",
    "\n",
    "    # Check if the negation is the same\n",
    "    if (not any(token.dep_ == 'neg' for token in doc1)) == (not any(token.dep_ == 'neg' for token in doc2)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def generate_negation_feature(q1, q2):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    negation_feature = []\n",
    "    for i in range(len(q1)):\n",
    "        neg = negation(q1[i],q2[i],nlp)\n",
    "        negation_feature.append(neg)\n",
    "    return negation_feature\n",
    "\n",
    "def test_negation():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    assert negation(\"Did you eat lunch?\", \"You didn't eat lunch?\", nlp) == 1\n",
    "    assert negation(\"Did you eat lunch?\", \"You ate lunch?\", nlp) == 0\n",
    "    assert negation(\"Are you happy?\", \"You're not happy?\", nlp) == 1\n",
    "    assert negation(\"Are you happy?\", \"You're happy?\", nlp) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820d880-11cb-41f9-aa12-064d301ca0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

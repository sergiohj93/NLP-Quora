{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.models.word2vec as w2v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'get_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39massert\u001b[39;00m embedding\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m2\u001b[39m, n_features)\n\u001b[1;32m     62\u001b[0m test_build_w2v_model()\n\u001b[0;32m---> 63\u001b[0m test_w2v_embedding()\n",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m, in \u001b[0;36mtest_w2v_embedding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m model \u001b[39m=\u001b[39m build_w2v_model(doc, n_features, n_epochs)\n\u001b[1;32m     55\u001b[0m \u001b[39m# When\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m embedding \u001b[39m=\u001b[39m w2v_embedding(doc, model)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Then\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(embedding, np\u001b[39m.\u001b[39mndarray)\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mw2v_embedding\u001b[0;34m(tokens, wv)\u001b[0m\n\u001b[1;32m     28\u001b[0m     word_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence:\n\u001b[0;32m---> 30\u001b[0m         word_vectors\u001b[39m.\u001b[39mappend(wv\u001b[39m.\u001b[39;49mget_vector(token))\n\u001b[1;32m     31\u001b[0m     sentence_vectors\u001b[39m.\u001b[39mappend(\u001b[39mlist\u001b[39m(np\u001b[39m.\u001b[39mmean(word_vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)))\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(sentence_vectors)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'get_vector'"
     ]
    }
   ],
   "source": [
    "def build_w2v_model(\n",
    "        tokens:list[list[str]],\n",
    "        n_features:int,\n",
    "        seed:int = 1,\n",
    "        workers = 1,\n",
    "        sg:int = 0,\n",
    "        context_size:int = 5,\n",
    "        down_sampling:int = 1e-3,\n",
    "        min_word_count:int = 0) -> w2v:\n",
    "\n",
    "    return w2v.Word2Vec(\n",
    "        sentences=tokens,\n",
    "        sg=sg,\n",
    "        seed=seed,\n",
    "        workers = workers,\n",
    "        vector_size = n_features,\n",
    "        min_count = min_word_count,\n",
    "        window = context_size,\n",
    "        sample = down_sampling\n",
    "    )\n",
    "\n",
    "def w2v_embedding(\n",
    "        tokens: list[list[str]], \n",
    "        wv: w2v.wv) -> np.ndarray:\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    for sentence in tokens:\n",
    "        word_vectors = []\n",
    "        for token in sentence:\n",
    "            word_vectors.append(wv.get_vector(token))\n",
    "        sentence_vectors.append(list(np.mean(word_vectors, axis=0)))\n",
    "\n",
    "    return np.array(sentence_vectors)\n",
    "\n",
    "def test_build_w2v_model():\n",
    "    # Given\n",
    "    doc = [['this', 'is', 'a', 'test'], ['this', 'is', 'another', 'test']]\n",
    "    n_features = 10\n",
    "    n_epochs = 10\n",
    "\n",
    "    # When\n",
    "    model = build_w2v_model(doc, n_features, n_epochs)\n",
    "\n",
    "    # Then\n",
    "    assert isinstance(model, w2v.Word2Vec)\n",
    "    assert model.vector_size == n_features\n",
    "\n",
    "def test_w2v_embedding():\n",
    "    # Given\n",
    "    doc = [['this', 'is', 'a', 'test'], ['this', 'is', 'another', 'test']]\n",
    "    n_features = 10\n",
    "    n_epochs = 10\n",
    "    model = build_w2v_model(doc, n_features, n_epochs)\n",
    "\n",
    "    # When\n",
    "    embedding = w2v_embedding(doc, model)\n",
    "\n",
    "    # Then\n",
    "    assert isinstance(embedding, np.ndarray)\n",
    "    assert embedding.shape == (2, n_features)\n",
    "\n",
    "test_build_w2v_model()\n",
    "test_w2v_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF and Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/6wn8yg3x7vxcc2xw91srqzv00000gn/T/ipykernel_49003/2498480371.py:12: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  if isinstance(vector1, scipy.sparse.csr.csr_matrix):\n"
     ]
    }
   ],
   "source": [
    "def tfidf_vectorizer(\n",
    "        corpus: list[str],\n",
    "        max_features=1000) -> TfidfVectorizer:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features)\n",
    "    return vectorizer.fit(corpus)\n",
    "\n",
    "\n",
    "def cosine_distance(\n",
    "        vector1: scipy.sparse.csr.csr_matrix | np.ndarray, \n",
    "        vector2: scipy.sparse.csr.csr_matrix | np.ndarray) -> float:\n",
    "    if isinstance(vector1, scipy.sparse.csr.csr_matrix):\n",
    "        return np.dot(\n",
    "            vector1.T.toarray()[0]/np.linalg.norm(vector1.toarray()),\n",
    "            vector2.T.toarray()[0]/np.linalg.norm(vector2.toarray()))\n",
    "    else:\n",
    "        return np.dot(\n",
    "            vector1/np.linalg.norm(vector1),\n",
    "            vector2/np.linalg.norm(vector2))\n",
    "    \n",
    "def test_tfidf_vectorizer():\n",
    "    # Given\n",
    "    doc = [\"This is a test\", \"This is another test\"]\n",
    "\n",
    "    # When\n",
    "    vectorizer = tfidf_vectorizer(doc)\n",
    "\n",
    "    # Then\n",
    "    assert isinstance(vectorizer, TfidfVectorizer)\n",
    "    assert (vectorizer.get_feature_names_out() == [\"another\", \"is\", \"test\", \"this\"]).all()\n",
    "\n",
    "def test_cosine_distance():\n",
    "    a = np.array([1, 0, 0])\n",
    "    b = np.array([0, 1, 0])\n",
    "    assert cosine_distance(a, b) == 0.0\n",
    "\n",
    "test_tfidf_vectorizer()\n",
    "test_cosine_distance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash Script Docker Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.5s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.7s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              0.8s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9              1.0s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (7/8)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9              1.1s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/library/python:3.9@sha256:2d8875d28ca023a9056a82  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] RUN git clone https://github.com/sergiohj93/NLP-Quora.gi  0.0s\n",
      "\u001b[0m => exporting to image                                                     0.0s\n",
      "\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m => => writing image sha256:3085613a840c05d3d7194ac700e687a34d2511951b0f7  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (8/8) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 205B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9              1.1s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/library/python:3.9@sha256:2d8875d28ca023a9056a82  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] RUN git clone https://github.com/sergiohj93/NLP-Quora.gi  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:3085613a840c05d3d7194ac700e687a34d2511951b0f7  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/nlp_quora:latest                        0.0s\n",
      "\u001b[?2004hb9226d6a4f76:/app# 6d6a4f76:/app# ^C\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004hroot@b9226d6a4f76:/app# "
     ]
    }
   ],
   "source": [
    "!bash docker-build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
